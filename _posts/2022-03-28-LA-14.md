---
layout: single
title: "일반최소제곱법과 QR분할"
excerpt: "일반화된 최소제곱법과 직교기저벡터를 이영한 QR분할을 알아보자."
categories: 선형대수학
tag: [일반최소제곱법, QR분할]
use_math: true
---

### Generalized Least square

임의의 행렬식이 주어지고 변수의 수보다 방정식의 갯수가 많을 때, 해를 구하지 못하기 때문에 Least square를 사용하여 최소거리를 구한다고 했다. 이번에는 조금 더 일반화된 Least square를 살펴보자.

![image-20220328162455081](https://raw.githubusercontent.com/kjw9899/kjw9899.github.io/master/kjw9899/kjw9899.github.io/assets/images/image-20220328162455081.png)

이 때 $x_i$를 $w_i$로 생각하면 된다. 즉 딥러닝에서 자주 볼 수 있는 가중치의 개념이라고 생각하면된다. Least square에서 가중치를 곱해줌으로써 이에 대한 최적의 최소거리를 찾기 위해 optimization을 거듭해준다. 



## Orthogonal Basis

orthogonal vector는 두가지의 특징을 가진다.

* linearly independent
* basis vectors

다음과 같은 orthonormal vector(정규직교벡터)가 있다고 하자.
$$
\begin{aligned}
q_1,\ q_2,\ \cdots\ ,\ q_n
\end{aligned}
$$
그리고 두 orthonormal vector를 내적을 하면,
$$
\begin{aligned}
q_i^Tq_j\ =\ \begin{cases} 0\ (i \neq\ j) \\ 1\ (i = j) \end{cases} 
\end{aligned}
$$
서로 다른 직교벡터를 내적하면 당연히 그 값은 0이고, 같은 직교벡터를 내적하면 그 값은 1이 됨을 알 수 있다.



이 때, orthonormal vector의 집합을 column space라고 생각하고 그것을 Q라고 했을 때, Q와 Transpose한 Q를 곱하면 반드시 identity vector가 될 것을 알 수 있다.

![image-20220328230648407](https://raw.githubusercontent.com/kjw9899/kjw9899.github.io/master/kjw9899/kjw9899.github.io/assets/images/image-20220328230648407.png)



> for Q $\in$ $\mathbb{R}^n$

Vector Space에 존재하는 n개의 orthonormal vector에 대해서 생각해보자. 위에서 정규직교벡터가 가지는 특징 중에 linearly independent한 특성이 있었다. 그 특성으로 또 다른 특성을 유도해보자.
$$
\begin{aligned}
\Rightarrow X\ &=\ \sum_{i=1}^n C_iq_i\\
X\ &=\ \begin{pmatrix} q_1\ q_2\ \cdots\ q_n \end{pmatrix}\ \begin{pmatrix}  C_1 \\ C_2 \\ \vdots\\ C_n  \end{pmatrix}
\end{aligned}
$$
각 계수 column vector는 다음과 같이 나타낼 수 있다.

![image-20220328231938541](https://raw.githubusercontent.com/kjw9899/kjw9899.github.io/master/kjw9899/kjw9899.github.io/assets/images/image-20220328231938541.png)

![image-20220328232011538](https://raw.githubusercontent.com/kjw9899/kjw9899.github.io/master/kjw9899/kjw9899.github.io/assets/images/image-20220328232011538.png)
$$
\begin{aligned}
c_i\ =\ \frac {q_i^TX} {q_i^Tq_i}
\end{aligned}
$$
계수 column vector는 다음과 같이 나타내진다(분모=0). 결국 projection 값이 된다.



$c_i$가 i 방향에 대해서 벡터 X의 내적을 구한 것을 의미한다. 그 내적은 크기 1인 q에 대한 것이므로 line projection과 같다. 서로 수직인 orthonormal vector로 basis를 구성하면 그 basis로 이루어지는 linear combination의 계수는 그 벡터가 각각의 basis vector들의 projection된 크기만큼으로 구성이 된다.

결국,
$$
\begin{aligned}
X\ =\ \sum_{j=1}^n C_jq_j\ =\ \sum_{i=1}^n (q_i^TX)q_i
\end{aligned}
$$
