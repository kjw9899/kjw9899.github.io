---
layout: single
title: "[논문리뷰]Simple Baselines for Human Pose Estimation and Tracking"
excerpt: ""
categories: HPE
tag: [Simple Baselines, Human Pose Estimation]
use_math: true
---

이번 포스팅에서는 Human Pose Estimation 기술 중에서 Top-down 방식에 해당하는 ***Simple Baseline***을 알아보고자 한다. SB는 단순한 network architecture로 높은 정확도를 이뤘으며 ResNet과 세 개의 deconvolutional layer의 조합으로 이루어져 있다. 더 구체적으로 알아보기 위해 리뷰를 시작한다.



> ***Abstract***

최근 몇 년간, pose estimation에 대한 기술이 급속도로 발전했다.  그에 대한 전반적인 알고리즘이나 system들의 복잡도 또한 증가했으며 알고리즘을 분석하거나 비교하고 만드는 것도 굉장히 어려워졌다. 

이 논문은 단순하고 효과적인  baseline method를 제공한다. 따라서 이 분야에서 새로운 idea들에게 영감이되길 소망한다.

Github Code : https://github.com/leoxiaobin/pose.pytorch.



## 1. Introduction

<center> <span style='background-color: #fff5b1'>"How good could a simple method be?"</span></center>

본 논문이 제안하는 pose estimation 기법은 단지 몇 개의 deconvolutional layers와 ResNet 기반의 Backbone으로 구성된다.  이것은 아마도 깊고 낮은 해상도의 feature map으로부터 추출되는 kp의 heatmap을 추정하는 방법 중에 가장 simple한 방법이다. 

단 하나의 $single$ 모델로 COCO testdev에서 mAP를 73.7을 기록하면서 2017년 keypoint Challenge's single model에서 우승을 차지했다. pose estimation이 빠르게 발전함에 따라 ***일상 생활에서 사람의 자세를 detection하고 tracking***하는 것이 굉장히 중요해졌다. 

맨 위의 질문의 대답으로 본 논문은 pose estimation and tracking에 대한 vaseline method를 제안한다. 이 방법은 매우 간단하지만 효율적이다. 



## 2. Pose Estimation Using A Deconvolution Head Network

![image-20220407224347019](https://raw.githubusercontent.com/kjw9899/kjw9899.github.io/master/kjw9899/kjw9899.github.io/assets/images/image-20220407224347019.png)

* Simple Baseline
  * $C_5$ : ResNet + a few deconvolution stage(orange color)
  * deep and low resolution feature로부터 얻어지는 heatmap을 만들어내는데 가장 simple하다.
  * 세 개의 deconvolutional layer에서 ReLu activation function과 Batch Nomalization이 사용되었다.
  * 각 레이어에 256개의 4x4 filter, stride = 2를 적용
  * 가장 마지막에 1x1 convolutional layer를 추가함으로써 각각의 keypoint $k$에 대한 heatmap을 생성한다.
  * predicted heatmap과 targeted heatmap의 차이는 MSE를 통해 구했다.
  * targeted heatmap에는 2D Gaussian을 이용하여 heatmap을 생성.

Fig 1에 있는 Hourglass Network나 CPN과 비교했을 때, Simple Baseline은 어떤식으로 high resolution feature가 생성되는지에 대한 궁금증이 있을 수 있다. 그에 대한 답은, upsampling과 covolutional parameter를 deconvolutional layer로 결합하는 데에 있다.



## 3. Pose Tracking Based on Optical Flow

![image-20220413173522553](https://raw.githubusercontent.com/kjw9899/kjw9899.github.io/master/kjw9899/kjw9899.github.io/assets/images/image-20220413173522553.png)

비디오에서 Multi-person pose tracking을 수행할 때는 프레임 단위 별로 진행한다. 그리고 각 프레임 별로 unique id를 할당함으로써 human pose를 추정한다. 

* Human instance $P = (J, id)$
* $J\ =\ {j_i}_{1:N_J}$ is coordinate set of $N_J$ body joints 
* $id$ is the tracking id

본 논문은 ICCV'17에서 multi-human pose에서 우승을 차지한 [Detect-and-track: Ecient pose estimation in videos](https://arxiv.org/abs/1712.09184)에서 두 가지 차이점을 두고 follow했다. 

첫 번째 차이점은 두 개의 다른 human bounding box를 사용한 것이다. bbox 중 하나는 human detector에서 추출된 것이고 또 다른 bbox는 optical flow를 사용한 이전의 프레임으로부터 만들어진 bbox이다.

두 번째 차이점은 greedy matching algorithm에 의해 사용된 similarity metric이다.



### 3.1 Joint Propagation using Optical Flow

![image-20220413180335445](https://raw.githubusercontent.com/kjw9899/kjw9899.github.io/master/kjw9899/kjw9899.github.io/assets/images/image-20220413180335445.png)



위의 그림과 같이 video의 프레임 사이에 flow field를 추가하여 벡터화된 kp좌표를 추출한다.



## 4. Experiment

![image-20220413204332834](https://raw.githubusercontent.com/kjw9899/kjw9899.github.io/master/kjw9899/kjw9899.github.io/assets/images/image-20220413204332834.png)

* COCO train, validation and test sets
* COCO train2017datasets



#### Train

* ground truth human box $\rightarrow$ height : width = 4 : 3 (default=256 : 192)
* scale($\pm$30%), rotation($\pm$40 degrees)
* ResNet Backbone network
* learning rate : 1e-3, drop to 1e-4 at 90 epochs and 1e-5 at 120 epoch.

* total epoch : 140
* Mini-batch size : 128 (GPU out of memory가 발생할 수 있으니 적절히 조정하는 것을 추천합니다.)
* Adam optimizer



#### Ablation Study(Table 2)

#### Heatmap

Method (a) : 64 x 48 히트맵을 생성하기 위한 3 deconvolutional layers 

Method (b) : 32 x 24 히트맵을 생성하기 위한 2 deconvolutional layers이다.

*Default : 3 deconvolutional layer*

#### Kernel size

Method (a, c, d)를 보면 kernel size가 작아질 수록 AP가 감소하는 것을 볼 수 있다. 

*Default : kernel size of 4*

#### Backbone

Table 2를 보면, Backbone 모델인 ResNet의 깊이가 깊을 수록 더 좋은 성능을 낸다는 것을 알 수 있다.

#### Image size 

 Method (a, g, h)를 보면 image size가 성능에 가장 큰 영향을 미친다는 것을 알 수 있다.



### Comparison with Other Methods on COCO val2017

![image-20220413215003477](https://raw.githubusercontent.com/kjw9899/kjw9899.github.io/master/kjw9899/kjw9899.github.io/assets/images/image-20220413215003477.png)

위 Table 3는 Hourglass, CPN 그리고 본 논문의 Simplebaseline을 비교한 것이다.













