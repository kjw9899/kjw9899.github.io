---
layout: single
title: "[논문리뷰]Deep High-Resolution Representation Learning for Visual Recognition"
excerpt: ""
categories: HPE
tag: [HRNet, HPE]
use_math: true
---

> ***Abstract***

High-resolution으로 이미지를 나타내는 것은 Human Pose Estimation에서 의미론적에서 필수적이다. 기존에 존재하는 ResNet, VGGNet과 같은 모델들은 encoding을 통해서 낮은 해상도로 표현하고 decoding으로 높은 해상도를 구현했다면,

본 논문이 제안하는 HRNet은 이미지 인풋이 모든 과정을 통과할 때까지 high-resolution을 유지한다. 그에 대한 두 가지의 핵심적인 특징이 있다.

* high-to-low resolution에 대한 convolution 층을 병렬로 연결한다.
* 병렬로 연결된 층을 통해 나온 정보들을 반복적으로 공유한다.

이에 대한 장점으로, 이미지의 의미론적인 이해가 충분하고 공간적인 정보량이 풍부하다는 것이다. 



## 1. Introduction

본 논문이 제안하는 가장 돋보이는 특징은 Network의 모든 과정을 거치더라도 high-resolution을 유지할 수 있다는 것이다. 처음에는 high-resolution convoluiton stream에서 시작하고 high-to-low resolution을 하나하나씩 추가해나간다. 그리고 multi-resolution을 병렬로 연결한다. 그 후에, 병렬로 연결된 stream의 정보를 계속해서 교환하면서 multi-resolution fusion을 진행한다.

![image-20220430123832088](https://raw.githubusercontent.com/kjw9899/kjw9899.github.io/master/kjw9899/kjw9899.github.io/assets/images/image-20220430123832088.png)

이러한 구조의 이득은 인풋의 의미를 잘 담아내고 공간적인 정보도 정교하게 얻을 수 있다는 것이다.

#### Two versions of HRNet

* HRNetV1
  * 고해상도 stream에서 연산된 고해상도 표현만 출력한다.
  * heatmap estimation framwork을 따르는 human pose estimation에 적용할 수 있다.
  * 경험적으로 COCO keypoint detection dataset에서 가장 좋은 성능을 거두었다.
* HRNetV2
  * 고/저 해상도 stream 병렬 stream으로부터 나온 representation을 결합한다.
  * semantic segmentation에 적용할 수 있다,
* HRNetVp2 (Additionally)
  * HRNetV2의 출력인 고해상도 stream으로부터의 다중해상도 표현이다.
  * Detection framework에 적용할 수 있다.



## 3. High-Resolution Network

### 3.1 Parallel Multi-Resolution Convolutions

stage1의 결과로 나중에 만들어지는 병렬 구조 stage의 resolution은 이전의 stage의 영향을 받아서 형성되며 크기는 줄어든다.

Fig 2에서 보여준 그림은 다음과 같은 과정을 따른다.

![image-20220430123611405](https://raw.githubusercontent.com/kjw9899/kjw9899.github.io/master/kjw9899/kjw9899.github.io/assets/images/image-20220430123611405.png)

* $N_{sr}$로 표현되고 s는 s번 째 stage, r은 resolution index를 뜻한다.
* r에 따라서 해상도가 결정되는데, 첫번 째 해상도를 기준으로 $\frac {1} {2^{r-1}}$에 비례하여 줄어든다.

### 3.2 Repeated Multi-Resolution Fusions

Fusion 모듈은 각각의 multi-resolution 표현들의 정보를 교차하면서 공유하는 것이다. 이 과정을 4번 반복한다. 

![image-20220430124344082](https://raw.githubusercontent.com/kjw9899/kjw9899.github.io/master/kjw9899/kjw9899.github.io/assets/images/image-20220430124344082.png)

* input : $\{ R^i_r\ ,\ r =\ 1,\ 2,\ 3 \}$
* output : $\{ R^o_r\ ,\ r =\ 1,\ 2,\ 3 \}$
* 각 output은 각 input의 합으로 나타난다. $R^o_r\  =\ f_{1r}(R^i_1)\ +\ f_{2r}(R^i_2)\ +\ f_{3r}(R^i_3)$
* stage3 부터 stage4까지의 아웃풋은 $R^o_4\ =\ f_{14}(R^i_1)\ +\ f_{24}(R^i_2)\ +\ f_{34}(R^i_3)$
* transform function인 $f_{xy}(\cdot)$은 input resolution index $x$와 output resolution index $r$에 의존한다.
* 만약 $x$와 $r$이 같다면, 해당 stage에서의 resolution 크기는 다음 stage의 resolution 크기와 동일하다. (Fig.3에서 볼 수 있듯이 파란색과 초록색 상자가 아닌 그대로 출력되는 것을 볼 수 있다.)
* 만약 $x$ < $r$이면, filter=3x3, stride=2를 가지는 convolutional 연산을 통해 downsampling된다.(Fig.3에서 초록색 상자)
* 만약 $x$ > $r$이면, 1x1 convolution을 통해 upsampling된다.

### 3.3 Representation Head

![image-20220430132057947](https://raw.githubusercontent.com/kjw9899/kjw9899.github.io/master/kjw9899/kjw9899.github.io/assets/images/image-20220430132057947.png)

* HRNetV1 (a) $\rightarrow$ *Human Pose Estimation*
  * 고해상도의 표현만 출력되고, 그를 제외한 나머지 세 개의 해상도 표현은 무시된다.
* HRNetV2 (b) $\rightarrow$ *Semantic Segmantation*
  * channel map의 갯수(depth)의 변경없이 bilinear upsampling을 통해서  low-resolution을 high-resolution으로 rescale한다. 
  * 4개의 representation을 concatenate하고 1x1 convolution을 통해 섞는다.
* HRNetV2p (c) $\rightarrow$ *Object Detection*
  * HRNetV2로부터 만들어진 high-resolution을 downsampling을 거치면서 여러 개의 resolution 표현으로 만든다.

## 4. Human Pose Estimation

![image-20220430143711500](https://raw.githubusercontent.com/kjw9899/kjw9899.github.io/master/kjw9899/kjw9899.github.io/assets/images/image-20220430143711500.png)

Human Pose Estimation (keypoint detection)은 W x H x 3의 크기를 가지는 image $I$에서 팔꿈치나 손목 등등의 사람의 관절 위치를 유추하는 것이다. 본 논문에서는 최신의 framework를 변형하여 $\frac {W} {4}$ x $\frac {H} {4}$ 크기를 가지는 히트맵 $K$을 측정한다. 그리고 각각의 히트맵 $H_k$는 관절이 있을만한 위치를 나타낸다.

본 논문에서는 HRNetV1의 output인 high-resolution 표현을 가지고 히트맵을 구한다. 많은 실험과 경험을 통해 HRNetV1과 HRNetV2의 성능이 거의 비슷하다는 도출했지만 연산량 측면에서 HRNetV1이 더 효율적이므로 HRNetV1을 선택했다고 한다. 

Loss function인 Mean Squared Error (MSE)을 통해 GT 히트맵과 pred 히트맵을 비교하는데 사용했다. 관절 keypoint를 중심으로 2개의 픽셀의 표준편차를 가지는 2D Gaussian을 통해서 GT 히트맵을 생성했다. 

![image-20220430144133223](https://raw.githubusercontent.com/kjw9899/kjw9899.github.io/master/kjw9899/kjw9899.github.io/assets/images/image-20220430144133223.png)

위 Table.1은 다른 모델들과 HRNetV1을 COCO val dataet으로 비교한 것이다. AP 측면에서 확연히 발전한 것을 볼 수 있으며 비교적 작은 모델인 HRNetV1-W32모델이 SimpleBaseline보다 더 좋은 성능을 보였다. 

* Dataset : COCO dataset 
* Evaluation metric : Object Keypoint Similarity (OKS)

$$
\begin{aligned}
OKS = \frac {\sum_{i}\ \exp(-d^2_i/2s^2k^2_i)\sigma(v_i > 0)} {\sum_i \sigma(v_i>0)}
\end{aligned}
$$

* $d_i$ : Ground Truth 키 포인트와 검출된 키포인트 사이의 Euclidean distance
* $v_i$ : Ground Truth의 visibility flag
* $s$ : object scale (객체 세그먼트 영역의 제곱근)
* $k_i$ : falloff를 제어하는 키 포인트마다 존재하는 상수

* Training
  * detection box $rightarrow$ height : width = 4 : 3
  * resized image $rightarrow$ 256 x 192 or 384 x 288
  * Adam optimizer
  * Learning rate : 1e-3 -(170th epoch)-> 1e-4 -(200th)-> 1e-5
  * teminated within 210 epochs



