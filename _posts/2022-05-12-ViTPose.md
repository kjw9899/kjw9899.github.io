---
layout: single
title: "[논문리뷰]VITPOSE: SIMPLE VISION TRANSFORMER BASELINES FOR HUMAN POSE ESTIMATION"
excerpt: ""
categories: HPE
tag: [ViTPOSE, Human Pose Estimation]
use_math: true
---

> ***Abstract***

최근 Vision Transformer는 많은 Human Pose estimation 기술에 적용되고 있으며, 매우 향상된 성능을 보이고 있다. 그러나, HPE에 vision transformer를 적용하는 것은 아직 불분명하다. 본 논문에서는 단순하고 비계층적인 vision transformer를 함께 사용하는 simple deconvolution decoder인 VitPose를 HPE에 적용한다.

미리 학습된 MAE(Masked Autoencoders)와 함께 사용한 plain(분명한) vision transformer는 finetuning된 HPE 데이터셋에서 높은 성능을 거둘 수 있다고 한다.

*MAE의 task는 이미지의 마스킹된 영역을 원본 이미지와 같도록 generate하는 것*

ViTPose는 model size와 관련하여 좋은 확장성을 가지고 있으며 입력 해상도과 토큰에 대한 유연성을 가지고 있다. 게다가, 그것은 라벨링 없는 pose data를 사용해서 쉽게 pretrain할 수 있다. ViTAE-G를 기반으로 한 가장 거대한 ViTPOSE 모델은 10억개의 parameter를 가지고 .COCO-dataset에서 최고 81.1의 mAP를 달성하였다. 

Github : https://github.com/ViTAE-Transformer/ViTPose.

## Introduction

* Pose recognition with cascade transformers, Li, 2021
  * encoder-decoder 구조를 통합하여 human keypoint를 점진적으로 예측한다.
* TokenPose, Li, 2021
  * CNN에 의한 feature extractor를 개선하기 위해서 부가적인 keypoint token을 경우에 맞게 사용하며 vision transformer를 적용했다.
* HRFormer, Yuan, 2021
  * 고해상도의 표현들을 다루기 위해서  multi-resolution을 얻기 위한 병렬구조의 vision transformer를 적용한다.

위에 소개한 기술들은 우수한 성능을 얻었지만, 여전히 HPE 작업이 이러한 수정없이 일반적인 vision transformer로부터 이점을 얻을 수 있을지 아직 불분명하다.

본 논문에서는 HPE를 위해서 단순하고 비계층적인 vision transformer를 채택하여 질문에 대한 답을 한다. 특히 저자는  <span style='background-color: #fff5b1'>keypoint heatmap을 회귀하기 위해서</span> vision transformer 뒤에 몇 개의 deconvolutional layers를 추가함으로서 HPE를 수행한다. 또한 더 좋은 성능을 위해서 model size, input resolution 그리고 token number와 관련된 ViTPose의 확장성과 유연성에 대해서 조사한다. 큰 규모의 upstream ImageNet dataset 없이 unlabeled pose data를 사용하여 쉽게 pretrain시킬 수 있다.



## 2. Method

### Simple vision transformer baseline

정교한 변동없이 HPE를 위해서 일반적인 vision transformer의 잠재력을 탐구한다. input($I$)와 마지막 transformer block의 feature($F$)는 다음과 같다.
$$
\begin{aligned}
I &\in R^{H \times\ W\ \times\ 3} \\
F &\in R^{\frac {H} {d}\ \times\ \frac {W} {d}\ \times\ C}
\end{aligned}
$$
$d$는 vision transformer의 downsampling 비율(전형적인 값은 16)이며, $C$는 channel dimension이다. 

HPE 방법의 루틴과 맞추기 위해, deconvolutional layer를 사용하여 feature map들을 input size의 1/4까지 upsampling한다. channel dimension은 각 deconvolutional layer에서 연산되는 costs를 줄이기 위해 256까지 downsampling한다. 그러한 ViT-Base backbone과 256 x 192의 input은 75.8 mAP를 기록했다.



### Pretraning

Vision Transformer는 더 좋은 성능을 위해 많은 data를 필요로 한다. 본 논문에서는 ImageNet-1K를 가지고 있는 pretrain되어 있는 MAE 로 vision transformer를 초기화한다.(약 100만 개의 이미지 data) 그리고 MS COCO의 약 50만 개의 사람의 이미지가 있다. 저자는 방금 언급한 것들이 필수적인가에 대해 의문을 품었다. 놀랍게도 MAE를 unlabeled pose data를 이용해서 pretraining했더니, simple ViTPose baseline이 Image-1K로 사전 학습한 것과 비슷한 결과를 보였다. 



### Finer-resolution feature maps

* simple vision transformer의 성능을 더욱 높이기 위해, HPE에서 finer-resolution feature map이 가지는 장점에 대해서 연구해보았다.
* 각 이미지 패치마다 downsampling 비율을 16으로 설정한 뒤, 이미지를 겹치지 않게끔 분할하여 input image의 1/16배로 downsampling한다.
* 그러면 각 패치는 channel $C$를 가지는 feature vector인 token으로 매핑되고 transformer block에 의해 처리된다.
* 최소한의 수정으로 finer-resolution feature map을 얻기 위해, padding으로 겹치는 patch를 사용한다.
* 예를 들어, downsampling 비율을 16에서 8로 증가시키고, 16 x 16 크기를 가지는 window로 이미지를 분할하고 인접한 window 사이에 8개의 픽셀이 겹치게끔한다.
* 비록 그러한 작업이 finer-resolution feature map를 만드는데 더욱 효과적이겠지만 이는 결국 연산량의 증가로 연결된다. 왜냐하면 transformer의 self-attention의 연산은 token의 제곱이기 때문이다.

## 3. Experiment

### Experiment settings

* COCO Keypoint detection dataset을 training과 test에 사용 (150k instance, 118k images)

* ImageNet dataset에서 MAE에 의해 사전 학습된 ViT-B를 채택했다. COCO  dataset으로 210 epoch

### The Influence of vatiations

![image-20220512124137509](https://raw.githubusercontent.com/kjw9899/kjw9899.github.io/master/kjw9899/kjw9899.github.io/assets/images/image-20220512124137509.png)

Table 1은 input image의 해상도를 다르게 했을 때 성능 차이를 보여준다. 표를 보면 알 수 있듯이 input image의 해상도가 높을 수록 HPE의 성능이 높아지는 것을 알 수 있다.



![image-20220512124400688](https://raw.githubusercontent.com/kjw9899/kjw9899.github.io/master/kjw9899/kjw9899.github.io/assets/images/image-20220512124400688.png)

Tabel 2는 pretraining dataset에 따른 성능 차이를 보여준다. vision transfomer는 데이터의 양과 질에 따라 성능에 영향을 크게 받는다. 표를 보면 알 수 있듯이, human pose data로 사전 학습한 모델과 ImageNet-1K로 학습한 모델의 성능이 거의 비슷하다. 

그 이외에도 모델 사이즈가 커질수록(parameter수가 많을 수록) 성능은 더더욱 좋아진다.

![image-20220512132129543](https://raw.githubusercontent.com/kjw9899/kjw9899.github.io/master/kjw9899/kjw9899.github.io/assets/images/image-20220512132129543.png)



## Conclusion

plain vision transformer는 humapose estimation에서 특별한 설계 과정없이 좋은 성능을 보였음을 증명했다. 모델 사이즈가 클수록, input image의 해상도가 높을수록, token number가 많을 수록 그 성능은 극대화된다. 본 저자는 이러한 연구가 다른 연구에 있어 유용한 시각을 제공하기를 바란다고 했다.





















